{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcFNfGmC6BO8OXb0qAaZe1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sk-gr/ml/blob/main/Refactoring_Python_DNN_with_core_torch_nn_components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab_idx = 1\n",
        "\n",
        "if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "    # path management for Python\n",
        "    pythonpath, = !echo $PYTHONPATH\n",
        "    if \".\" not in pythonpath.split(\":\"):\n",
        "        pythonpath = \".:\" + pythonpath\n",
        "        %env PYTHONPATH={pythonpath}\n",
        "        !echo $PYTHONPATH\n",
        "\n",
        "    # get both Colab and local notebooks into the same state\n",
        "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
        "    import bootstrap\n",
        "\n",
        "    # change into the lab directory\n",
        "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
        "\n",
        "    # allow \"hot-reloading\" of modules\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "    # needed for inline plots in some contexts\n",
        "    %matplotlib inline\n",
        "\n",
        "    bootstrap.run = False  # change to True re-run setup\n",
        "\n",
        "!pwd\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOkZ5lCuSDD3",
        "outputId": "3b579345-1d46-433a-c2c6-ef1cb4e09e38"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=.:/env/python\n",
            ".:/env/python\n",
            "/content/fsdl-text-recognizer-2022-labs/lab01\n",
            "\u001b[0m\u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34mtext_recognizer\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsokTZTMJ3x6"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "\n",
        "def download_mnist(path):\n",
        "    url = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "    filename = \"mnist.pkl.gz\"\n",
        "\n",
        "    if not (path / filename).exists():\n",
        "        content = requests.get(url + filename).content\n",
        "        (path / filename).open(\"wb\").write(content)\n",
        "\n",
        "    return path / filename\n",
        "\n",
        "\n",
        "data_path = Path(\"data\") if Path(\"data\").exists() else Path(\"../data\")\n",
        "path = data_path / \"downloaded\" / \"vector-mnist\"\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "datafile = download_mnist(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "\n",
        "def read_mnist(path):\n",
        "    with gzip.open(path, \"rb\") as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
        "    return x_train, y_train, x_valid, y_valid\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = read_mnist(datafile)"
      ],
      "metadata": {
        "id": "ITR3jCuQ9ZHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")\n",
        "\n",
        "print(x_train, y_train, sep=\"\\n\")\n",
        "\n",
        "y_train[0], x_train[0, ::2]\n",
        "\n",
        "x_train.ndim, y_train.ndim\n",
        "\n",
        "n, c = x_train.shape\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf1uvPb49ayo",
        "outputId": "b15aa0a3-7c3f-4f0d-e0c1-f7c009fcf578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([5, 0, 4,  ..., 8, 4, 8])\n",
            "torch.Size([50000, 784])\n",
            "torch.Size([50000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-93ee5ed35295>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_train, y_train, x_valid, y_valid = map(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-execute this cell for more samples\n",
        "import random\n",
        "\n",
        "import wandb  # just for some convenience methods that convert tensors to human-friendly datatypes\n",
        "\n",
        "import text_recognizer.metadata.mnist as metadata # metadata module holds metadata separate from data\n",
        "\n",
        "idx = random.randint(0, len(x_train))\n",
        "example = x_train[idx]\n",
        "\n",
        "print(y_train[idx])  # the label of the image\n",
        "wandb.Image(example.reshape(*metadata.DIMS)).image  # the image itself"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "id": "XjGgR-MmX-yk",
        "outputId": "e1d40b48-3446-455d-b470-ee3340c7bf95"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7ED27F6508B0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABs0lEQVR4nO2Uv+sBcRjHn9xFugwkMpDSDXKl5A9QNqMo6YYbTsiEQdkMJpP/wGQwMfg1o5tNBpuBuDDIj6t7+A5K3+TXt7tM39f6ed6v3sPzeQD++TKxWKzdbl8ul9VqFQgESJJUaiRJcjab4S94nlcqTSQSiNjpdGiartVqsiwvFgu32/02qHnxZjAYAOBwOEynU47jBEGwWCz5fF6R9I5mswkAHo/n88gDjEajJEn7/d5utwMAy7KIKAjC2+CrptvtNhKJyLKcTqcBIJlMAoAoioqaXmEYxul0AsBgMEBElmVVkF7x+/2SJG02G6/Xq46RIIhWq4WIpVJJHSMAUBR1XX6GYVSTZrNZRByPxxRFqWM0m82j0QgROY5TxwgA4XAYEUVRpGlaNWm/3z+dTj6f78VMLpcjCOJTo8PhWC6Xu93u2YDL5apUKo1GQ6P5+MeXy2VEfCi12WypVGo+n3e73buteHN0TSYTAGi12lAoNJlMAMBqtUajUb1ez/P88Xis1+uZTOZ8Pn9aEwCKxSI+Yb1eB4PBP7hu6HS6QqEwHA5vrl6vV61W4/H49XT98yV+AHBw2xHU8053AAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwBGOACn4uoE",
        "outputId": "5c77db20-a290-4bf4-b611-ec7d41bd4036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.9845, -2.2457, -2.2202, -2.9017, -2.4166, -2.3481, -2.3291, -2.3609,\n",
            "        -1.9366, -2.6244], grad_fn=<SelectBackward0>) torch.Size([64, 10])\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "\n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)\n",
        "\n",
        "def linear(x: torch.Tensor) -> torch.Tensor:\n",
        "    return x @ weights + bias\n",
        "\n",
        "def log_softmax(x: torch.Tensor) -> torch.Tensor:\n",
        "    return x - torch.log(torch.sum(torch.exp(x), axis=1))[:, None]\n",
        "\n",
        "def model(xb: torch.Tensor) -> torch.Tensor:\n",
        "    return log_softmax(linear(xb))\n",
        "\n",
        "bs = 64  # batch size\n",
        "\n",
        "xb = x_train[0:bs]  # a batch of inputs\n",
        "outs = model(xb)  # outputs on that batch\n",
        "\n",
        "print(outs[0], outs.shape)  # outputs on the first element of the batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(out: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "metadata": {
        "id": "UUdbPiI7-B4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb = y_train[0:bs]\n",
        "\n",
        "acc = accuracy(outs, yb)\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvBtoD66I3xt",
        "outputId": "c1cdffd4-7cba-4499-f229-666364b168f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0469)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    acc.backward()\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du4rdUPYKMWe",
        "outputId": "c9fde16f-d187-40cc-e397-3d46aac612f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    return -output[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = cross_entropy"
      ],
      "metadata": {
        "id": "nHNYUjMbLGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(outs, yb), -torch.log(torch.tensor(1 / 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEEQJx7GLUQX",
        "outputId": "a5f2ce50-7a16-4819-f4cd-399504fa6021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3562, grad_fn=<NegBackward0>) tensor(2.3026)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_func(outs, yb)\n",
        "\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "_b9mVEtsLjZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YifGm5vzLmao",
        "outputId": "a12b51be-341d-4650-8fee-a56f491d9a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0320, -0.0325,  0.0466, -0.0593, -0.0263,  0.0282,  0.0103,  0.0199,\n",
              "         0.0320, -0.0509])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywWny8xwMkUB",
        "outputId": "4c684006-ece1-4c52-a6ea-552485283844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0827, grad_fn=<NegBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.5  # learning rate hyperparameter\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs):  # loop over the data repeatedly\n",
        "    for ii in range((n - 1) // bs + 1):  # in batches of size bs, so roughly n / bs of them\n",
        "        start_idx = ii * bs  # we are ii batches in, each of size bs\n",
        "        end_idx = start_idx + bs  # and we want the next bs entires\n",
        "\n",
        "        # pull batches from x and from y\n",
        "        xb = x_train[start_idx:end_idx]\n",
        "        yb = y_train[start_idx:end_idx]\n",
        "\n",
        "        # run model\n",
        "        pred = model(xb)\n",
        "\n",
        "        # get loss\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        # calculate the gradients with a backwards pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters\n",
        "        with torch.no_grad():  # we don't want to track gradients through this part!\n",
        "            # SGD learning rule: update with negative gradient scaled by lr\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "\n",
        "            # ACHTUNG: PyTorch doesn't assume you're done with gradients\n",
        "            #          until you say so -- by explicitly \"deleting\" them,\n",
        "            #          i.e. setting the gradients to 0.\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()"
      ],
      "metadata": {
        "id": "v4IOGK7ULzmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "    return xb @ weights + bias"
      ],
      "metadata": {
        "id": "qXn43NzaNy4R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))  # should be unchanged from above!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrqiLEflOEc1",
        "outputId": "161a43e0-8cb8-4877-f39e-44a9437b2bae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0827, grad_fn=<NllLossBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class MNISTLogistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()  # the nn.Module.__init__ method does import setup, so this is mandatory\n",
        "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
        "        self.bias = nn.Parameter(torch.zeros(10))"
      ],
      "metadata": {
        "id": "XOS3hmMROJ8n"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, xb: torch.Tensor) -> torch.Tensor:\n",
        "    return xb @ self.weights + self.bias\n",
        "\n",
        "MNISTLogistic.forward = forward\n",
        "\n",
        "model = MNISTLogistic()  # instantiated as an object\n",
        "print(model(xb)[:4])  # callable like a function\n",
        "loss = loss_func(model(xb), yb)  # composable like a function\n",
        "loss.backward()  # we can still take gradients through it\n",
        "print(model.weights.grad[::17,::2])  # and they show up in the .grad attribute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGxt_mqROKdG",
        "outputId": "b2b06e30-caf9-4ad6-cf4f-ecd2a5c5012f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1781, -0.5651, -0.0313,  0.1976, -0.0446, -0.3763,  0.5980, -0.5332,\n",
            "         -0.3749, -0.1366],\n",
            "        [ 0.2445, -0.5270,  0.1161, -0.1365,  0.4139,  0.0527,  0.6680,  0.2940,\n",
            "         -0.9837, -0.0812],\n",
            "        [ 0.1678, -0.3692, -0.0647, -0.1343,  0.1190, -0.4345,  0.6747,  0.1757,\n",
            "          0.0255,  0.0274],\n",
            "        [-0.0444, -0.8932, -0.2095,  0.3465,  0.0835,  0.1170,  0.4541,  0.5866,\n",
            "         -0.5395, -0.0260]], grad_fn=<SliceBackward0>)\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 9.1295e-03,  7.6317e-03,  1.0138e-02,  1.0776e-02,  3.1128e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-3.1918e-02, -1.7080e-02,  6.0463e-02,  7.2544e-02, -1.5655e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.9081e-02,  1.7784e-02, -3.5617e-02,  2.1078e-02, -4.4636e-02],\n",
            "        [ 2.0167e-02,  1.9904e-02,  2.9291e-02,  2.8722e-02, -5.9581e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-6.9675e-02,  1.6502e-02,  4.6517e-02,  6.5739e-02, -3.9460e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-9.4888e-02,  2.3986e-02,  6.7591e-03,  2.8926e-02, -2.1847e-02],\n",
            "        [-5.7116e-02,  5.4039e-02,  6.6332e-02,  7.5699e-02, -1.3236e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.8187e-02,  2.4154e-02,  3.4074e-02,  3.6524e-02, -5.8074e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-5.4759e-02,  1.0822e-02,  5.4499e-03,  6.1139e-03,  4.6453e-03],\n",
            "        [ 2.0700e-02,  1.7614e-02, -5.3370e-02,  3.1664e-02, -4.4645e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.2129e-02,  3.7659e-02, -1.0768e-02,  6.7349e-02, -8.5795e-02],\n",
            "        [ 7.3240e-03,  5.5689e-03, -5.3447e-02,  8.1696e-03,  6.4722e-03],\n",
            "        [-2.6999e-02,  5.9988e-03,  3.3556e-03,  3.7409e-03,  2.7205e-03],\n",
            "        [-1.4972e-02, -2.1163e-02, -4.9920e-02,  7.4323e-02, -1.3050e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.9996e-02, -1.5709e-02, -3.6217e-02,  8.4703e-02, -1.0310e-01],\n",
            "        [-5.3446e-02,  6.7734e-03,  9.0447e-03,  8.1487e-03,  4.9365e-03],\n",
            "        [-1.3111e-03,  1.2003e-04,  1.6156e-04,  1.3864e-04,  9.8751e-05],\n",
            "        [-5.3097e-02,  3.6665e-02,  5.5023e-03,  5.6664e-02, -1.2482e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.4803e-02,  3.1616e-02,  6.2874e-03,  7.4695e-02, -8.8276e-02],\n",
            "        [-5.3540e-02,  6.6975e-03,  8.9775e-03,  7.9932e-03,  4.8762e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.0230e-01,  1.0103e-02,  8.9417e-02,  1.1057e-01, -1.3381e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.6731e-02,  1.4632e-02,  1.8091e-02,  2.2937e-02, -4.9957e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 8.9397e-04,  6.2649e-04,  7.4673e-04,  1.2889e-03,  6.5686e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*list(model.parameters()), sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv1uaVipO9xC",
        "outputId": "c0e616f3-a542-4e73-dfc5-0762bd826b75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0227, -0.0234,  0.0283,  ..., -0.0156, -0.0457,  0.0340],\n",
            "        [ 0.0055, -0.0035,  0.0368,  ...,  0.0205, -0.0277, -0.0223],\n",
            "        [-0.0024, -0.0102,  0.0545,  ..., -0.0282, -0.0437,  0.0199],\n",
            "        ...,\n",
            "        [-0.0358, -0.0084,  0.0212,  ..., -0.0338, -0.0056,  0.0136],\n",
            "        [-0.0032, -0.0090,  0.0117,  ..., -0.0075,  0.0010, -0.0719],\n",
            "        [-0.0183,  0.0083, -0.0028,  ...,  0.0281,  0.0451, -0.0180]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "q9NxJZTOJ3yG"
      },
      "outputs": [],
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for ii in range((n - 1) // bs + 1):\n",
        "            start_idx = ii * bs\n",
        "            end_idx = start_idx + bs\n",
        "            xb = x_train[start_idx:end_idx]\n",
        "            yb = y_train[start_idx:end_idx]\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():  # finds params automatically\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "\n",
        "fit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KAyyDLNPEdQ",
        "outputId": "d33dff0d-c43e-45ab-f518-55b055427550"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "print(\"torch.nn.Modules:\", *textwrap.wrap(\", \".join(torch.nn.modules.__all__)), sep=\"\\n\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hHHITqIQ8Nz",
        "outputId": "aa99f0d9-b199-4a79-f0dd-10187ac0e2e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.nn.Modules:\n",
            "\tModule, Identity, Linear, Conv1d, Conv2d, Conv3d, ConvTranspose1d,\n",
            "\tConvTranspose2d, ConvTranspose3d, Threshold, ReLU, Hardtanh, ReLU6,\n",
            "\tSigmoid, Tanh, Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GLU,\n",
            "\tGELU, Hardshrink, LeakyReLU, LogSigmoid, Softplus, Softshrink,\n",
            "\tMultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU,\n",
            "\tL1Loss, NLLLoss, KLDivLoss, MSELoss, BCELoss, BCEWithLogitsLoss,\n",
            "\tNLLLoss2d, PoissonNLLLoss, CosineEmbeddingLoss, CTCLoss,\n",
            "\tHingeEmbeddingLoss, MarginRankingLoss, MultiLabelMarginLoss,\n",
            "\tMultiLabelSoftMarginLoss, MultiMarginLoss, SmoothL1Loss,\n",
            "\tGaussianNLLLoss, HuberLoss, SoftMarginLoss, CrossEntropyLoss,\n",
            "\tContainer, Sequential, ModuleList, ModuleDict, ParameterList,\n",
            "\tParameterDict, AvgPool1d, AvgPool2d, AvgPool3d, MaxPool1d, MaxPool2d,\n",
            "\tMaxPool3d, MaxUnpool1d, MaxUnpool2d, MaxUnpool3d, FractionalMaxPool2d,\n",
            "\tFractionalMaxPool3d, LPPool1d, LPPool2d, LocalResponseNorm,\n",
            "\tBatchNorm1d, BatchNorm2d, BatchNorm3d, InstanceNorm1d, InstanceNorm2d,\n",
            "\tInstanceNorm3d, LayerNorm, GroupNorm, SyncBatchNorm, Dropout,\n",
            "\tDropout1d, Dropout2d, Dropout3d, AlphaDropout, FeatureAlphaDropout,\n",
            "\tReflectionPad1d, ReflectionPad2d, ReflectionPad3d, ReplicationPad2d,\n",
            "\tReplicationPad1d, ReplicationPad3d, CrossMapLRN2d, Embedding,\n",
            "\tEmbeddingBag, RNNBase, RNN, LSTM, GRU, RNNCellBase, RNNCell, LSTMCell,\n",
            "\tGRUCell, PixelShuffle, PixelUnshuffle, Upsample, UpsamplingNearest2d,\n",
            "\tUpsamplingBilinear2d, PairwiseDistance, AdaptiveMaxPool1d,\n",
            "\tAdaptiveMaxPool2d, AdaptiveMaxPool3d, AdaptiveAvgPool1d,\n",
            "\tAdaptiveAvgPool2d, AdaptiveAvgPool3d, TripletMarginLoss, ZeroPad2d,\n",
            "\tConstantPad1d, ConstantPad2d, ConstantPad3d, Bilinear,\n",
            "\tCosineSimilarity, Unfold, Fold, AdaptiveLogSoftmaxWithLoss,\n",
            "\tTransformerEncoder, TransformerDecoder, TransformerEncoderLayer,\n",
            "\tTransformerDecoderLayer, Transformer, LazyLinear, LazyConv1d,\n",
            "\tLazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d,\n",
            "\tLazyConvTranspose3d, LazyBatchNorm1d, LazyBatchNorm2d,\n",
            "\tLazyBatchNorm3d, LazyInstanceNorm1d, LazyInstanceNorm2d,\n",
            "\tLazyInstanceNorm3d, Flatten, Unflatten, Hardsigmoid, Hardswish, SiLU,\n",
            "\tMish, TripletMarginWithDistanceLoss, ChannelShuffle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTLogistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784, 10)  # pytorch finds the nn.Parameters inside this nn.Module\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb)  # call nn.Linear.forward here"
      ],
      "metadata": {
        "id": "ew5jEiuKQ9dn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "print(loss_func(model(xb), yb))  # loss is still close to 2.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY3Seq2wRFdB",
        "outputId": "e8b6c5b2-0b87-4f9a-e054-d10fe0a8025f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2346, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*list(model.children()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XARZqolRJR6",
        "outputId": "62ba8205-9ba7-4bed-f283-793db30b50b2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=784, out_features=10, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(*list(model.parameters()), sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuVSr6XERR4b",
        "outputId": "59970bf5-44ee-44dd-9a2e-e01fac3d9022"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0060,  0.0284,  0.0328,  ...,  0.0236,  0.0084, -0.0055],\n",
            "        [ 0.0224,  0.0170, -0.0151,  ...,  0.0126, -0.0323,  0.0296],\n",
            "        [-0.0242, -0.0030, -0.0137,  ...,  0.0073, -0.0313,  0.0039],\n",
            "        ...,\n",
            "        [-0.0133,  0.0056,  0.0116,  ..., -0.0208,  0.0299,  0.0163],\n",
            "        [ 0.0222, -0.0321,  0.0272,  ..., -0.0008, -0.0167,  0.0114],\n",
            "        [ 0.0349,  0.0239, -0.0048,  ..., -0.0186,  0.0119,  0.0233]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([ 0.0038,  0.0309,  0.0311, -0.0297,  0.0222,  0.0321,  0.0075, -0.0030,\n",
            "         0.0050,  0.0319], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
        "    return optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "4_3HA9czRXIc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "opt = configure_optimizer(model)\n",
        "\n",
        "print(\"before training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for ii in range((n - 1) // bs + 1):\n",
        "        start_idx = ii * bs\n",
        "        end_idx = start_idx + bs\n",
        "        xb = x_train[start_idx:end_idx]\n",
        "        yb = y_train[start_idx:end_idx]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(\"after training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3G8_H-rRhUV",
        "outputId": "84430881-f7e4-48f2-a87e-36fcad0ab9e9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before training:\n",
            "\ttensor(2.4524, grad_fn=<NllLossBackward0>)\n",
            "after training:\n",
            "\ttensor(0.8417, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from text_recognizer.data.util import BaseDataset\n",
        "\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train)"
      ],
      "metadata": {
        "id": "BS-hTFImR0I4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BaseDataset??"
      ],
      "metadata": {
        "id": "sVrKlZslSP65"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "opt = configure_optimizer(model)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for ii in range((n - 1) // bs + 1):\n",
        "        xb, yb = train_ds[ii * bs: ii * bs + bs]  # xb and yb in one line!\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KyFxX0zSQUq",
        "outputId": "e2850c06-d7d1-48de-c3c0-46314670da16"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8639, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=bs)"
      ],
      "metadata": {
        "id": "8Iyb_HrRSd0W"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self: nn.Module, train_dataloader: DataLoader):\n",
        "    opt = configure_optimizer(self)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xb, yb in train_dataloader:\n",
        "            pred = self(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "MNISTLogistic.fit = fit"
      ],
      "metadata": {
        "id": "obwY85VSSvgA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "\n",
        "model.fit(train_dataloader)\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc4Dj9CTS8iM",
        "outputId": "9546638a-05c5-4e3c-91ce-3160eb024fd9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8492, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from text_recognizer.models.mlp import MLP\n",
        "\n",
        "\n",
        "MLP.fit = fit  # attach our fitting loop"
      ],
      "metadata": {
        "id": "r2D47mDbT7uu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP.forward??"
      ],
      "metadata": {
        "id": "Y4VmIez5UKGn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP.__init__??"
      ],
      "metadata": {
        "id": "1VL1zBP5URYW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digits_to_9 = list(range(10))\n",
        "data_config = {\"input_dims\": (784,), \"mapping\": {digit: str(digit) for digit in digits_to_9}}\n",
        "data_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6-hLMNLUVSa",
        "outputId": "f78a03b6-7aed-484e-c72f-3cca8fda9b87"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_dims': (784,),\n",
              " 'mapping': {0: '0',\n",
              "  1: '1',\n",
              "  2: '2',\n",
              "  3: '3',\n",
              "  4: '4',\n",
              "  5: '5',\n",
              "  6: '6',\n",
              "  7: '7',\n",
              "  8: '8',\n",
              "  9: '9'}}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(data_config)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l05xDcjLUhAl",
        "outputId": "cb75e74b-89fe-4238-91ac-e18f0d09cb2f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Mxomx4Uwcv",
        "outputId": "3b7292c8-5569-49ec-862e-5a227a9e0256"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0168, -0.0066,  0.0221,  ...,  0.0214, -0.0126,  0.0088],\n",
              "        [ 0.0347, -0.0178, -0.0268,  ..., -0.0236,  0.0246, -0.0100],\n",
              "        [ 0.0181, -0.0151,  0.0192,  ..., -0.0234,  0.0252,  0.0079],\n",
              "        ...,\n",
              "        [ 0.0244, -0.0243, -0.0292,  ...,  0.0260,  0.0354, -0.0351],\n",
              "        [-0.0297, -0.0184, -0.0120,  ...,  0.0073, -0.0075,  0.0336],\n",
              "        [ 0.0029,  0.0281,  0.0158,  ..., -0.0302,  0.0251,  0.0205]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(\"before training:\", loss_func(model(xb), yb))\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=bs)\n",
        "fit(model, train_dataloader)\n",
        "\n",
        "print(\"after training:\", loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFjuw1jhUxZz",
        "outputId": "f208b0fa-f1b0-4c69-b367-e441f76a2b8e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before training: tensor(2.3279, grad_fn=<NllLossBackward0>)\n",
            "after training: tensor(0.3664, grad_fn=<NllLossBackward0>)\n",
            "CPU times: user 12.9 s, sys: 4.84 ms, total: 12.9 s\n",
            "Wall time: 13.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"☹️\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHzQI4rtVzGh",
        "outputId": "65ca5898-864a-4a9f-c145-c3d617e5820b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "☹️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "loss_func(model(xb.to(device)), yb.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtC71_tSWAs9",
        "outputId": "9cddb831-51f4-4a3e-dd6a-579d8786cb4b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2467, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_device(tensor):\n",
        "    return tensor.to(device)\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train, transform=push_to_device, target_transform=push_to_device)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=bs)"
      ],
      "metadata": {
        "id": "jPjDt3vhWRrH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = MLP(data_config)\n",
        "model.to(device)\n",
        "\n",
        "model.fit(train_dataloader)\n",
        "\n",
        "print(loss_func(model(push_to_device(xb)), push_to_device(yb)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-4eq7AVWxgn",
        "outputId": "1dffd79b-d4e6-48bb-84c1-d813132a5386"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2300, grad_fn=<NllLossBackward0>)\n",
            "CPU times: user 13.1 s, sys: 8.76 ms, total: 13.1 s\n",
            "Wall time: 13.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule:\n",
        "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "    filename = \"mnist.pkl.gz\"\n",
        "\n",
        "    def __init__(self, dir, bs=32):\n",
        "        self.dir = dir\n",
        "        self.bs = bs\n",
        "        self.path = self.dir / self.filename\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not (self.path).exists():\n",
        "            content = requests.get(self.url + self.filename).content\n",
        "            self.path.open(\"wb\").write(content)\n",
        "\n",
        "    def setup(self):\n",
        "        with gzip.open(self.path, \"rb\") as f:\n",
        "            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
        "\n",
        "        x_train, y_train, x_valid, y_valid = map(\n",
        "            torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        "            )\n",
        "\n",
        "        self.train_ds = BaseDataset(x_train, y_train, transform=push_to_device, target_transform=push_to_device)\n",
        "        self.valid_ds = BaseDataset(x_valid, y_valid, transform=push_to_device, target_transform=push_to_device)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_ds, batch_size=self.bs, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.valid_ds, batch_size=2 * self.bs, shuffle=False)"
      ],
      "metadata": {
        "id": "iWn8tyYCW7Md"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self: nn.Module, datamodule):\n",
        "    datamodule.prepare_data()\n",
        "    datamodule.setup()\n",
        "\n",
        "    val_dataloader = datamodule.val_dataloader()\n",
        "\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
        "\n",
        "    print(\"before start of training:\", valid_loss / len(val_dataloader))\n",
        "\n",
        "    opt = configure_optimizer(self)\n",
        "    train_dataloader = datamodule.train_dataloader()\n",
        "    for epoch in range(epochs):\n",
        "        self.train()\n",
        "        for xb, yb in train_dataloader:\n",
        "            pred = self(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
        "\n",
        "        print(epoch, valid_loss / len(val_dataloader))\n",
        "\n",
        "\n",
        "MNISTLogistic.fit = fit\n",
        "MLP.fit = fit"
      ],
      "metadata": {
        "id": "3TLBHKOyXf3g"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(data_config)\n",
        "model.to(device)\n",
        "\n",
        "datamodule = MNISTDataModule(dir=path, bs=32)\n",
        "\n",
        "model.fit(datamodule=datamodule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "_egBy-6mXsBB",
        "outputId": "6268b240-166e-424b-d266-7105cb2a496f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e6b4f41e4322>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdatamodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-41bba2c5e70b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, datamodule)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-5aa83df4e1a8>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         return self._accessor.open(self, mode, buffering, encoding, errors,\n\u001b[0m\u001b[1;32m   1120\u001b[0m                                    newline)\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/downloaded/vector-mnist/mnist.pkl.gz'"
          ]
        }
      ]
    }
  ]
}